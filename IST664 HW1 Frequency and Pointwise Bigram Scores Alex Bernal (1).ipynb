{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T17:12:28.316781Z",
     "start_time": "2019-07-21T17:12:28.312794Z"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import nltk\n",
    "from nltk import FreqDist\n",
    "from nltk.corpus import gutenberg\n",
    "\n",
    "import nltk\n",
    "import random\n",
    "import re\n",
    "\n",
    "import pprint \n",
    "import os\n",
    "import numpy\n",
    "import nltk.corpus\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "from nltk import cluster\n",
    "from nltk.cluster import util\n",
    "from nltk.cluster import api\n",
    "from nltk.cluster import euclidean_distance\n",
    "from nltk.cluster import cosine_distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data : Document 1 EMMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T18:23:45.814938Z",
     "start_time": "2019-07-21T18:23:45.805962Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "897451"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emma = open(\"emma.txt\")\n",
    "rawtextE = emma.read()\n",
    "len(rawtextE) #192427"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T18:25:51.646071Z",
     "start_time": "2019-07-21T18:25:50.782380Z"
    }
   },
   "outputs": [],
   "source": [
    "emmatokens = nltk.word_tokenize(rawtextE)\n",
    "emmawords = [w.lower( ) for w in emmatokens] \n",
    "\n",
    "#import string\n",
    "#table = str.maketrans('', '', string.punctuation)\n",
    "#emmawords = [w.translate(table) for w in emmaLower]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examine Document 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T18:25:52.839055Z",
     "start_time": "2019-07-21T18:25:52.737357Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<FreqDist with 8215 samples and 193789 outcomes>\n"
     ]
    }
   ],
   "source": [
    "# Creating a frequency distribution of words\n",
    "ndist = FreqDist(emmawords)\n",
    "print(ndist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stopwords Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T18:28:45.653045Z",
     "start_time": "2019-07-21T18:28:45.649054Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "199\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\", '_', 'could', 'would', 'might', 'must', 'need', 'sha', 'wo', 'y', \"'s\", \"'d\", \"'ll\", \"'t\", \"'m\", \"'re\", \"'ve\", \"n't\", 'mrs.', 'mr.', 'miss']\n"
     ]
    }
   ],
   "source": [
    "# get a list of stopwords from nltk\n",
    "nltkstopwords = nltk.corpus.stopwords.words('english')\n",
    "morestopwords = ['_','could','would','might','must','need','sha','wo','y',\"'s\",\"'d\",\"'ll\",\"'t\",\"'m\",\"'re\",\"'ve\", \"n't\",\"mrs.\",\"mr.\",\"miss\"]\n",
    "stopwords = nltkstopwords + morestopwords\n",
    "print(len(stopwords))\n",
    "print(stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to match non-alphabetical characters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T18:27:58.336760Z",
     "start_time": "2019-07-21T18:27:58.333742Z"
    }
   },
   "outputs": [],
   "source": [
    "def alpha_filter(w):\n",
    "  # pattern to match word of non-alphabetical characters\n",
    "  pattern = re.compile('^[^a-z]+$')\n",
    "  if (pattern.match(w)):\n",
    "    return True\n",
    "  else:\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization : Document 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T18:28:00.035299Z",
     "start_time": "2019-07-21T18:27:59.549535Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['******the', 'project', 'gutenberg', 'etext', 'of', 'emma', ',', 'by', 'jane', 'austen******', 'please', 'take', 'a', 'look', 'at', 'the', 'important', 'information', 'in', 'this', 'header', '.', 'we', 'encourage', 'you', 'to', 'keep', 'this', 'file', 'on', 'your', 'own', 'disk', ',', 'keeping', 'an', 'electronic', 'path', 'open', 'for', 'the', 'next', 'reader', '.', 'do', 'not', 'remove', 'this', '.', '**welcome', 'to', 'the', 'world', 'of', 'free', 'plain', 'vanilla', 'electronic', 'texts**', '**etexts', 'readable', 'by', 'both', 'human', 'and', 'by', 'computer', ',', 'since', '1971**', '*these', 'etexts', 'prepared', 'by', 'hundred', 'of', 'volunteer', 'and', 'donations*', 'information', 'on', 'contacting', 'project', 'gutenberg', 'to', 'get', 'etexts', ',', 'and', 'further', 'information', 'is', 'included', 'below', '.', 'we', 'need', 'your', 'donation', '.']\n"
     ]
    }
   ],
   "source": [
    "wnl = nltk.WordNetLemmatizer()\n",
    "emmaLemma = [wnl.lemmatize(t) for t in emmawords]\n",
    "print(emmaLemma[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigrams by Raw Frequency - Document 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T18:28:48.163950Z",
     "start_time": "2019-07-21T18:28:47.551534Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Bigrams from file with top 50 frequencies\n",
      "(('frank', 'churchill'), 0.0007430762323970917)\n",
      "(('every', 'thing'), 0.0006398712001197179)\n",
      "(('every', 'body'), 0.0005624674259116874)\n",
      "(('jane', 'fairfax'), 0.0005418264194562127)\n",
      "(('young', 'man'), 0.00042830088395110145)\n",
      "(('emma', 'wa'), 0.0003302561032875963)\n",
      "(('great', 'deal'), 0.0003302561032875963)\n",
      "(('said', 'emma'), 0.0003044548452182528)\n",
      "(('john', 'knightley'), 0.0002838138387627781)\n",
      "(('dare', 'say'), 0.0002631728323073033)\n",
      "(('wa', 'quite'), 0.00022189081939635378)\n",
      "(('young', 'lady'), 0.0002064100645547477)\n",
      "(('weston', 'wa'), 0.00018060880648540422)\n",
      "(('dear', 'emma'), 0.00017028830325766686)\n",
      "(('harriet', 'smith'), 0.00015996780002992947)\n",
      "(('maple', 'grove'), 0.00015996780002992947)\n",
      "(('robert', 'martin'), 0.00015996780002992947)\n",
      "(('elton', 'wa'), 0.00015480754841606077)\n",
      "(('harriet', 'wa'), 0.00014964729680219208)\n",
      "(('colonel', 'campbell'), 0.00013932679357445469)\n",
      "(('cried', 'emma'), 0.00013932679357445469)\n",
      "(('young', 'woman'), 0.000134166541960586)\n",
      "(('wa', 'always'), 0.00012384603873284863)\n",
      "(('wa', 'obliged'), 0.00012384603873284863)\n",
      "(('body', 'else'), 0.00011868578711897992)\n",
      "(('depend', 'upon'), 0.00010836528389124254)\n",
      "(('good', 'deal'), 0.00010836528389124254)\n",
      "(('emma', 'felt'), 9.804478066350515e-05)\n",
      "(('box', 'hill'), 9.288452904963646e-05)\n",
      "(('take', 'care'), 9.288452904963646e-05)\n",
      "(('much', 'better'), 8.772427743576776e-05)\n",
      "(('woodhouse', 'wa'), 8.772427743576776e-05)\n",
      "(('every', 'day'), 8.256402582189908e-05)\n",
      "(('last', 'night'), 8.256402582189908e-05)\n",
      "(('wa', 'going'), 8.256402582189908e-05)\n",
      "(('wa', 'soon'), 8.256402582189908e-05)\n",
      "(('dear', 'jane'), 7.740377420803039e-05)\n",
      "(('knightley', 'wa'), 7.740377420803039e-05)\n",
      "(('let', 'u'), 7.740377420803039e-05)\n",
      "(('poor', 'harriet'), 7.740377420803039e-05)\n",
      "(('project', 'gutenberg'), 7.740377420803039e-05)\n",
      "(('replied', 'emma'), 7.740377420803039e-05)\n",
      "(('soon', 'afterwards'), 7.740377420803039e-05)\n",
      "(('wa', 'rather'), 7.740377420803039e-05)\n",
      "(('wa', 'still'), 7.740377420803039e-05)\n",
      "(('ever', 'since'), 7.224352259416169e-05)\n",
      "(('wa', 'really'), 7.224352259416169e-05)\n",
      "(('dear', 'sir'), 6.7083270980293e-05)\n",
      "(('great', 'pleasure'), 6.7083270980293e-05)\n",
      "(('next', 'day'), 6.7083270980293e-05)\n"
     ]
    }
   ],
   "source": [
    "# setup to process bigrams\n",
    "from nltk.collocations import *\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "       \n",
    "finder = BigramCollocationFinder.from_words(emmaLemma)\n",
    "# choose to use both the non-alpha word filter and a stopwords filter\n",
    "finder.apply_word_filter(alpha_filter)\n",
    "finder.apply_word_filter(lambda w: w in stopwords)\n",
    "\n",
    "# score by frequency and display the top 50 bigrams\n",
    "scored = finder.score_ngrams(bigram_measures.raw_freq)\n",
    "print ()\n",
    "print (\"Bigrams from file with top 50 frequencies\")\n",
    "for item in scored[:50]:\n",
    "        print (item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bigrams my Mutual Information Score : Document 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T18:29:03.985813Z",
     "start_time": "2019-07-21T18:29:03.366460Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Bigrams from file with top 50 frequencies\n",
      "(('.set', 'bin'), 17.564127156150366)\n",
      "(('26th', 'ult.'), 17.564127156150366)\n",
      "(('_a_', '_source_'), 17.564127156150366)\n",
      "(('_amor_', '_patriae_'), 17.564127156150366)\n",
      "(('_and_', '_misery_'), 17.564127156150366)\n",
      "(('_any_', '_thing_'), 17.564127156150366)\n",
      "(('_be_', '_a_'), 17.564127156150366)\n",
      "(('_caro_', '_sposo_'), 17.564127156150366)\n",
      "(('_dissolved_', '_it_.'), 17.564127156150366)\n",
      "(('_great_', '_way_'), 17.564127156150366)\n",
      "(('_most_', '_precious_'), 17.564127156150366)\n",
      "(('_precious_', '_treasures_'), 17.564127156150366)\n",
      "(('_repentance_', '_and_'), 17.564127156150366)\n",
      "(('_rev._', '_philip_'), 17.564127156150366)\n",
      "(('_robin_', '_adair_'), 17.564127156150366)\n",
      "(('_small_', 'half-glass'), 17.564127156150366)\n",
      "(('_with_', '_time_'), 17.564127156150366)\n",
      "(('adequate', 'restorative'), 17.564127156150366)\n",
      "(('anonymous', 'password'), 17.564127156150366)\n",
      "(('b.', 'kramer'), 17.564127156150366)\n",
      "(('baronne', \"d'almane\"), 17.564127156150366)\n",
      "(('base', 'aspersion'), 17.564127156150366)\n",
      "(('charles', 'b.'), 17.564127156150366)\n",
      "(('christened', 'catherine'), 17.564127156150366)\n",
      "(('clear-sighted', 'goodwill.'), 17.564127156150366)\n",
      "(('coarser', 'featured'), 17.564127156150366)\n",
      "(('comtesse', \"d'ostalis\"), 17.564127156150366)\n",
      "(('corrupt', 'data'), 17.564127156150366)\n",
      "(('dated', 'sept.'), 17.564127156150366)\n",
      "(('daughter-in-law', 'elect'), 17.564127156150366)\n",
      "(('de', 'genlis'), 17.564127156150366)\n",
      "(('designedly', 'suppress'), 17.564127156150366)\n",
      "(('dexterously', 'throwing'), 17.564127156150366)\n",
      "(('dispiriting', 'cogitation'), 17.564127156150366)\n",
      "(('en', 'passant'), 17.564127156150366)\n",
      "(('espalier', 'apple-trees'), 17.564127156150366)\n",
      "(('finest-looking', 'home-baked'), 17.564127156150366)\n",
      "(('gold', 'reticule'), 17.564127156150366)\n",
      "(('goodly', 'heritage'), 17.564127156150366)\n",
      "(('gutenberg-', 'tm'), 17.564127156150366)\n",
      "(('idlest', 'haunt'), 17.564127156150366)\n",
      "(('inconveniently', 'shy'), 17.564127156150366)\n",
      "(('infamous', 'fraud'), 17.564127156150366)\n",
      "(('inspect', 'anything'), 17.564127156150366)\n",
      "(('italian', 'singing.'), 17.564127156150366)\n",
      "(('kitchen', 'chimney'), 17.564127156150366)\n",
      "(('leather', 'gaiter'), 17.564127156150366)\n",
      "(('madame', 'de'), 17.564127156150366)\n",
      "(('malt', 'liquor'), 17.564127156150366)\n",
      "(('mechanically', 'twisted'), 17.564127156150366)\n"
     ]
    }
   ],
   "source": [
    "finder = BigramCollocationFinder.from_words(emmaLemma)\n",
    "# choose to use both the non-alpha word filter and a stopwords filter\n",
    "finder.apply_word_filter(alpha_filter)\n",
    "finder.apply_word_filter(lambda w: w in stopwords)\n",
    "\n",
    "# score by frequency and display the top 50 bigrams\n",
    "scored = finder.score_ngrams(bigram_measures.pmi)\n",
    "print ()\n",
    "print (\"Bigrams from file with top 50 frequencies\")\n",
    "for item in scored[:50]:\n",
    "        print (item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T17:27:08.317114Z",
     "start_time": "2019-07-21T17:27:08.312154Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "112310"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "caeser = open(\"shakespeare-caesar.txt\")\n",
    "rawtextC = caeser.read()\n",
    "len(rawtextC) #112310"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization : Document 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T17:29:12.941681Z",
     "start_time": "2019-07-21T17:29:12.801052Z"
    }
   },
   "outputs": [],
   "source": [
    "caesertokens = nltk.word_tokenize(rawtextC)\n",
    "caeserwords = [w.lower( ) for w in caesertokens] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine Document 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T17:29:34.447378Z",
     "start_time": "2019-07-21T17:29:34.432407Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<FreqDist with 3091 samples and 25251 outcomes>\n"
     ]
    }
   ],
   "source": [
    "# Creating a frequency distribution of words\n",
    "ndist2 = FreqDist(caeserwords)\n",
    "print(ndist2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmitization Document 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T17:30:28.184005Z",
     "start_time": "2019-07-21T17:30:28.119178Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[', 'the', 'tragedie', 'of', 'julius', 'caesar', 'by', 'william', 'shakespeare', '1599', ']', 'actus', 'primus', '.', 'scoena', 'prima', '.', 'enter', 'flauius', ',', 'murellus', ',', 'and', 'certaine', 'commoner', 'ouer', 'the', 'stage', '.', 'flauius', '.', 'hence', ':', 'home', 'you', 'idle', 'creature', ',', 'get', 'you', 'home', ':', 'is', 'this', 'a', 'holiday', '?', 'what', ',', 'know', 'you', 'not', '(', 'being', 'mechanicall', ')', 'you', 'ought', 'not', 'walke', 'vpon', 'a', 'labouring', 'day', ',', 'without', 'the', 'signe', 'of', 'your', 'profession', '?', 'speake', ',', 'what', 'trade', 'art', 'thou', '?', 'car', '.', 'why', 'sir', ',', 'a', 'carpenter', 'mur', '.', 'where', 'is', 'thy', 'leather', 'apron', ',', 'and', 'thy', 'rule', '?', 'what', 'dost']\n"
     ]
    }
   ],
   "source": [
    "wnl = nltk.WordNetLemmatizer()\n",
    "caeserLemma = [wnl.lemmatize(t) for t in caeserwords]\n",
    "print(caeserLemma[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigrams by Raw Frequency - Document 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T17:31:29.546892Z",
     "start_time": "2019-07-21T17:31:29.437186Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Bigrams from file with top 50 frequencies\n",
      "(('let', 'v'), 0.0006336382717516138)\n",
      "(('mark', 'antony'), 0.0005148310957981862)\n",
      "(('marke', 'antony'), 0.00047522870381371034)\n",
      "(('thou', 'art'), 0.00043562631182923446)\n",
      "(('art', 'thou'), 0.00035642152786028277)\n",
      "(('enter', 'brutus'), 0.00035642152786028277)\n",
      "(('noble', 'brutus'), 0.00035642152786028277)\n",
      "(('thou', 'hast'), 0.00035642152786028277)\n",
      "(('caesar', 'caes'), 0.0003168191358758069)\n",
      "(('good', 'morrow'), 0.0003168191358758069)\n",
      "(('good', 'night'), 0.0003168191358758069)\n",
      "(('haue', 'done'), 0.0003168191358758069)\n",
      "(('lord', 'bru'), 0.0003168191358758069)\n",
      "(('antony', 'ant'), 0.000277216743891331)\n",
      "(('enter', 'lucius'), 0.000277216743891331)\n",
      "(('come', 'downe'), 0.00023761435190685517)\n",
      "(('euery', 'man'), 0.00023761435190685517)\n",
      "(('great', 'caesar'), 0.00023761435190685517)\n",
      "(('haue', 'seene'), 0.00023761435190685517)\n",
      "(('caesar', 'shall'), 0.00019801195992237932)\n",
      "(('caesar', 'wa'), 0.00019801195992237932)\n",
      "(('caius', 'cassius'), 0.00019801195992237932)\n",
      "(('caius', 'ligarius'), 0.00019801195992237932)\n",
      "(('decius', 'brutus'), 0.00019801195992237932)\n",
      "((\"did'st\", 'thou'), 0.00019801195992237932)\n",
      "(('enter', 'antony'), 0.00019801195992237932)\n",
      "(('fell', 'downe'), 0.00019801195992237932)\n",
      "(('haue', 'beene'), 0.00019801195992237932)\n",
      "(('haue', 'heard'), 0.00019801195992237932)\n",
      "(('honourable', 'men'), 0.00019801195992237932)\n",
      "(('metellus', 'cymber'), 0.00019801195992237932)\n",
      "(('mine', 'owne'), 0.00019801195992237932)\n",
      "(('shall', 'finde'), 0.00019801195992237932)\n",
      "(('shall', 'haue'), 0.00019801195992237932)\n",
      "(('wa', 'ambitious'), 0.00019801195992237932)\n",
      "(('caesar', 'doth'), 0.00015840956793790345)\n",
      "(('caesar', 'hath'), 0.00015840956793790345)\n",
      "(('euery', 'one'), 0.00015840956793790345)\n",
      "(('hee', 'put'), 0.00015840956793790345)\n",
      "(('heere', 'come'), 0.00015840956793790345)\n",
      "(('honourable', 'man'), 0.00015840956793790345)\n",
      "(('l', 'heare'), 0.00015840956793790345)\n",
      "(('messala', 'messa'), 0.00015840956793790345)\n",
      "(('mine', 'eye'), 0.00015840956793790345)\n",
      "(('noble', 'antony'), 0.00015840956793790345)\n",
      "(('noble', 'caesar'), 0.00015840956793790345)\n",
      "(('tell', 'thee'), 0.00015840956793790345)\n",
      "(('thou', 'shalt'), 0.00015840956793790345)\n",
      "(('wilt', 'thou'), 0.00015840956793790345)\n",
      "(('ye', 'god'), 0.00015840956793790345)\n"
     ]
    }
   ],
   "source": [
    "# setup to process bigrams\n",
    "from nltk.collocations import *\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "       \n",
    "finder = BigramCollocationFinder.from_words(caeserLemma)\n",
    "# choose to use both the non-alpha word filter and a stopwords filter\n",
    "finder.apply_word_filter(alpha_filter)\n",
    "finder.apply_word_filter(lambda w: w in stopwords)\n",
    "\n",
    "# score by frequency and display the top 50 bigrams\n",
    "scored = finder.score_ngrams(bigram_measures.raw_freq)\n",
    "print ()\n",
    "print (\"Bigrams from file with top 50 frequencies\")\n",
    "for item in scored[:50]:\n",
    "        print (item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bigrams my Mutual Information Score : Document 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T17:32:34.267735Z",
     "start_time": "2019-07-21T17:32:34.158031Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Bigrams from file with top 50 frequencies\n",
      "(('attendant', 'absent'), 14.62405290271976)\n",
      "(('ayre-lesse', 'dungeon'), 14.62405290271976)\n",
      "(('beaten', 'brasse'), 14.62405290271976)\n",
      "(('bench', '4.ple'), 14.62405290271976)\n",
      "(('blacke', 'sentence'), 14.62405290271976)\n",
      "(('bussling', 'rumor'), 14.62405290271976)\n",
      "(('carry', 'anger'), 14.62405290271976)\n",
      "(('cauerne', 'darke'), 14.62405290271976)\n",
      "(('chimney', 'top'), 14.62405290271976)\n",
      "(('comet', 'seen'), 14.62405290271976)\n",
      "(('commended', 'beauty'), 14.62405290271976)\n",
      "(('craues', 'warie'), 14.62405290271976)\n",
      "(('cynicke', 'rime'), 14.62405290271976)\n",
      "(('dart', 'inuenomed'), 14.62405290271976)\n",
      "(('deceitfull', 'iades'), 14.62405290271976)\n",
      "(('dis-ioynes', 'remorse'), 14.62405290271976)\n",
      "(('disturbed', 'skie'), 14.62405290271976)\n",
      "(('diuers', \"sland'rous\"), 14.62405290271976)\n",
      "(('domesticke', 'fury'), 14.62405290271976)\n",
      "(('dumbe', 'mouthes'), 14.62405290271976)\n",
      "(('et', 'tu'), 14.62405290271976)\n",
      "(('falles', 'shrewdly'), 14.62405290271976)\n",
      "(('fine', 'workman'), 14.62405290271976)\n",
      "(('flearing', 'tell-tale'), 14.62405290271976)\n",
      "(('foure', 'wench'), 14.62405290271976)\n",
      "(('grey', 'line'), 14.62405290271976)\n",
      "(('hastie', 'sparke'), 14.62405290271976)\n",
      "(('high-sighted-tyranny', 'range'), 14.62405290271976)\n",
      "(('impatiently', 'stampt'), 14.62405290271976)\n",
      "(('knottie', 'oakes'), 14.62405290271976)\n",
      "(('latest', 'seruice'), 14.62405290271976)\n",
      "(('lofty', 'scene'), 14.62405290271976)\n",
      "(('loss', 'shold'), 14.62405290271976)\n",
      "(('lowly', 'courtesy'), 14.62405290271976)\n",
      "(('melancholy', 'childe'), 14.62405290271976)\n",
      "(('naked', 'breast'), 14.62405290271976)\n",
      "(('natiue', 'semblance'), 14.62405290271976)\n",
      "(('neighbor', 'showted'), 14.62405290271976)\n",
      "(('northerne', 'starre'), 14.62405290271976)\n",
      "(('obey', 'necessitie'), 14.62405290271976)\n",
      "(('ordered', 'honourably'), 14.62405290271976)\n",
      "(('otherwise', 'bethinke'), 14.62405290271976)\n",
      "(('owe', 'mo'), 14.62405290271976)\n",
      "(('peeuish', 'school-boy'), 14.62405290271976)\n",
      "(('pre-formed', 'faculty'), 14.62405290271976)\n",
      "(('pretor', 'chayre'), 14.62405290271976)\n",
      "(('pulling', 'scarffes'), 14.62405290271976)\n",
      "(('rabblement', 'howted'), 14.62405290271976)\n",
      "(('rascall', 'counter'), 14.62405290271976)\n",
      "(('richest', 'alchymie'), 14.62405290271976)\n"
     ]
    }
   ],
   "source": [
    "finder = BigramCollocationFinder.from_words(caeserLemma)\n",
    "# choose to use both the non-alpha word filter and a stopwords filter\n",
    "finder.apply_word_filter(alpha_filter)\n",
    "finder.apply_word_filter(lambda w: w in stopwords)\n",
    "\n",
    "# score by frequency and display the top 50 bigrams\n",
    "scored = finder.score_ngrams(bigram_measures.pmi)\n",
    "print ()\n",
    "print (\"Bigrams from file with top 50 frequencies\")\n",
    "for item in scored[:50]:\n",
    "        print (item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What are the topic comparison of 2 Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T17:34:27.876891Z",
     "start_time": "2019-07-21T17:34:27.873870Z"
    }
   },
   "outputs": [],
   "source": [
    "docE = emmaLemma\n",
    "docC = caeserLemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T17:35:43.914925Z",
     "start_time": "2019-07-21T17:35:43.903954Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document 1 Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T17:53:46.018342Z",
     "start_time": "2019-07-21T17:53:45.619437Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<193789x3941 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 62627 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "Etm = cv.fit_transform(docE)\n",
    "Etm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T17:51:41.766983Z",
     "start_time": "2019-07-21T17:51:41.705121Z"
    }
   },
   "source": [
    "# LDA Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T17:53:47.997375Z",
     "start_time": "2019-07-21T17:53:47.604426Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<193789x3941 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 62627 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "Etm = cv.fit_transform(docE)\n",
    "Etm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T17:55:05.323743Z",
     "start_time": "2019-07-21T17:54:00.659653Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "             evaluate_every=-1, learning_decay=0.7,\n",
       "             learning_method='batch', learning_offset=10.0,\n",
       "             max_doc_update_iter=100, max_iter=10, mean_change_tol=0.001,\n",
       "             n_components=7, n_jobs=None, n_topics=None, perp_tol=0.1,\n",
       "             random_state=42, topic_word_prior=None,\n",
       "             total_samples=1000000.0, verbose=0)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "LDA = LatentDirichletAllocation(n_components=7,random_state=42)\n",
    "# This can take awhile, we're dealing with a large amount of documents!\n",
    "LDA.fit(Etm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T17:56:08.102673Z",
     "start_time": "2019-07-21T17:56:08.079706Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "really\n",
      "hartfield\n",
      "better\n",
      "come\n",
      "man\n",
      "ha\n",
      "quite\n",
      "know\n",
      "thing\n",
      "harriet\n"
     ]
    }
   ],
   "source": [
    "single_topic = LDA.components_[0]\n",
    "top_word_indices = single_topic.argsort()[-10:]\n",
    "for index in top_word_indices:\n",
    "    print(cv.get_feature_names()[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T17:56:10.468234Z",
     "start_time": "2019-07-21T17:56:10.313648Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE TOP 15 WORDS FOR TOPIC #0\n",
      "['doubt', 'mean', 'home', 'long', 'woman', 'really', 'hartfield', 'better', 'come', 'man', 'ha', 'quite', 'know', 'thing', 'harriet']\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #1\n",
      "['looked', 'martin', 'place', 'subject', 'felt', 'sort', 'heard', 'pleasure', 'wish', 'like', 'frank', 'fairfax', 'day', 'elton', 'mr']\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #2\n",
      "['hour', 'morning', 'idea', 'happy', 'away', 'bates', 'feeling', 'young', 'body', 'sure', 'father', 'churchill', 'dear', 'jane', 'said']\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #3\n",
      "['believe', 'feel', 'going', 'highbury', 'love', 'room', 'poor', 'letter', 'way', 'make', 'friend', 'say', 'did', 'good', 'miss']\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #4\n",
      "['campbell', 'hand', 'carriage', 'eye', 'knew', 'smith', 'deal', 'right', 'till', 'moment', 'having', 'hope', 'soon', 'knightley', 'emma']\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #5\n",
      "['john', 'evening', 'house', 'party', 'manner', 'doe', 'mind', 'look', 'just', 'shall', 'great', 'woodhouse', 'little', 'think', 'weston']\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #6\n",
      "['perry', 'possible', 'let', 'child', 'want', 'half', 'lady', 'came', 'yes', 'word', 'oh', 'thought', 'time', 'mrs', 'wa']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for index,topic in enumerate(LDA.components_):\n",
    "    print(f'THE TOP 15 WORDS FOR TOPIC #{index}')\n",
    "    print([cv.get_feature_names()[i] for i in topic.argsort()[-15:]])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document 2 Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T17:56:52.822203Z",
     "start_time": "2019-07-21T17:56:52.760397Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<25251x1143 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 8678 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "Ctm = cv.fit_transform(docC)\n",
    "Ctm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T17:57:45.989811Z",
     "start_time": "2019-07-21T17:57:37.309044Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "             evaluate_every=-1, learning_decay=0.7,\n",
       "             learning_method='batch', learning_offset=10.0,\n",
       "             max_doc_update_iter=100, max_iter=10, mean_change_tol=0.001,\n",
       "             n_components=7, n_jobs=None, n_topics=None, perp_tol=0.1,\n",
       "             random_state=42, topic_word_prior=None,\n",
       "             total_samples=1000000.0, verbose=0)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "LDA2 = LatentDirichletAllocation(n_components=7,random_state=42)\n",
    "# This can take awhile, we're dealing with a large amount of documents!\n",
    "LDA2.fit(Ctm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T17:58:13.077516Z",
     "start_time": "2019-07-21T17:58:13.069541Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ca\n",
      "roman\n",
      "vp\n",
      "giue\n",
      "rome\n",
      "caes\n",
      "day\n",
      "friend\n",
      "men\n",
      "shall\n"
     ]
    }
   ],
   "source": [
    "single_topic = LDA2.components_[0]\n",
    "top_word_indices = single_topic.argsort()[-10:]\n",
    "for index in top_word_indices:\n",
    "    print(cv.get_feature_names()[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T17:58:19.060397Z",
     "start_time": "2019-07-21T17:58:19.019507Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE TOP 15 WORDS FOR TOPIC #0\n",
      "['marke', 'hee', 'lucius', 'god', 'make', 'ca', 'roman', 'vp', 'giue', 'rome', 'caes', 'day', 'friend', 'men', 'shall']\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #1\n",
      "['fellow', 'way', 'master', 'finde', 'liue', 'forth', 'till', 'exeunt', 'cinna', 'speake', 'man', 'enter', 'know', 'come', 'thou']\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #2\n",
      "['honor', 'shew', 'peace', 'bid', 'thinke', 'euery', 'doth', 'time', 'word', 'hath', 'noble', 'selfe', 'like', 'hand', 'wa']\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #3\n",
      "['away', 'downe', 'octauius', 'great', 'blood', 'looke', 'luc', 'death', 'stand', 'tell', 'vpon', 'thee', 'antony', 'haue', 'caesar']\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #4\n",
      "['face', 'dye', 'euer', 'true', 'wrong', 'titinius', 'sir', 'caska', 'thing', 'brut', 'night', 'heere', 'let', 'good', 'brutus']\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #5\n",
      "['best', 'vnto', 'reason', 'por', 'house', 'euen', 'messa', 'spirit', 'beare', 'messala', 'heare', 'st', 'ant', 'did', 'cassius']\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #6\n",
      "['octa', 'world', 'place', 'leaue', 'eye', 'art', 'feare', 'loue', 'heart', 'say', 'cask', 'lord', 'thy', 'cassi', 'bru']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for index,topic in enumerate(LDA2.components_):\n",
    "    print(f'THE TOP 15 WORDS FOR TOPIC #{index}')\n",
    "    print([cv.get_feature_names()[i] for i in topic.argsort()[-15:]])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emma Without Lemmaziation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T18:36:50.776495Z",
     "start_time": "2019-07-21T18:36:49.639509Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['project', 'gutenberg', 'etext', 'emma', 'jane', 'austen', 'please', 'take', 'look', 'important', 'information', 'header', 'encourage', 'keep', 'file', 'disk', 'keeping', 'electronic', 'path', 'open', 'next', 'readers', 'remove', 'welcome', 'world', 'free', 'plain', 'vanilla', 'electronic', 'texts', 'etexts', 'readable', 'humans', 'computers', 'since', 'etexts', 'prepared', 'hundreds', 'volunteers', 'donations', 'information', 'contacting', 'project', 'gutenberg', 'get', 'etexts', 'information', 'included', 'donations', 'emma', 'jane', 'austen', 'august', 'etext', 'date', 'last', 'updated', 'august', 'project', 'gutenberg', 'etext', 'emma', 'jane', 'austen', 'file', 'named', 'corrected', 'editions', 'etexts', 'get', 'new', 'number', 'versions', 'based', 'separate', 'sources', 'get', 'new', 'letter', 'trying', 'release', 'books', 'one', 'month', 'advance', 'official', 'release', 'dates', 'time', 'better', 'editing', 'official', 'release', 'date', 'project', 'gutenberg', 'etexts', 'midnight', 'central', 'time']\n"
     ]
    }
   ],
   "source": [
    "#Emma Without Lemmaziation\n",
    "# load data\n",
    "filename = 'emma.txt'\n",
    "file = open(filename, 'rt')\n",
    "text = file.read()\n",
    "file.close()\n",
    "# split into words\n",
    "from nltk.tokenize import word_tokenize\n",
    "tokens = word_tokenize(text)\n",
    "# convert to lower case\n",
    "tokens = [w.lower() for w in tokens]\n",
    "# remove punctuation from each word\n",
    "import string\n",
    "table = str.maketrans('', '', string.punctuation)\n",
    "stripped = [w.translate(table) for w in tokens]\n",
    "# remove remaining tokens that are not alphabetic\n",
    "words = [word for word in stripped if word.isalpha()]\n",
    "# filter out stop words\n",
    "nltkstopwords = nltk.corpus.stopwords.words('english')\n",
    "morestopwords = ['_','could','would','might','must','need','sha','wo','y',\"'s\",\"'d\",\"'ll\",\"'t\",\"'m\",\"'re\",\"'ve\", \"n't\",\"mrs\",\"mr\",\"miss\"]\n",
    "stopwords = nltkstopwords + morestopwords\n",
    "words = [w for w in words if not w in stopwords]\n",
    "print(words[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T18:36:52.934907Z",
     "start_time": "2019-07-21T18:36:52.633688Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Bigrams from file with top 50 frequencies\n",
      "(('frank', 'churchill'), 0.0021436383521691578)\n",
      "(('every', 'thing'), 0.0018374043018592782)\n",
      "(('jane', 'fairfax'), 0.0016040831206707983)\n",
      "(('every', 'body'), 0.0015895005468465184)\n",
      "(('young', 'man'), 0.0012249362012395188)\n",
      "(('said', 'emma'), 0.0009478672985781991)\n",
      "(('great', 'deal'), 0.000933284724753919)\n",
      "(('john', 'knightley'), 0.0008312067079839591)\n",
      "(('dare', 'say'), 0.0007437112650382793)\n",
      "(('colonel', 'campbell'), 0.0005395552314983595)\n",
      "(('dear', 'emma'), 0.0004812249362012395)\n",
      "(('harriet', 'smith'), 0.0004520597885526795)\n",
      "(('maple', 'grove'), 0.0004520597885526795)\n",
      "(('robert', 'martin'), 0.0004520597885526795)\n",
      "(('upon', 'word'), 0.0004228946409041196)\n",
      "(('cried', 'emma'), 0.0003937294932555596)\n",
      "(('oh', 'yes'), 0.0003645643456069996)\n",
      "(('young', 'woman'), 0.0003645643456069996)\n",
      "(('dear', 'woodhouse'), 0.0003499817717827196)\n",
      "(('body', 'else'), 0.0003353991979584397)\n",
      "(('young', 'lady'), 0.0003353991979584397)\n",
      "(('depend', 'upon'), 0.0003208166241341597)\n",
      "(('emma', 'felt'), 0.0003062340503098797)\n",
      "(('good', 'deal'), 0.0003062340503098797)\n",
      "(('said', 'knightley'), 0.0003062340503098797)\n",
      "(('much', 'better'), 0.0002916514764855997)\n",
      "(('said', 'weston'), 0.0002916514764855997)\n",
      "(('oh', 'dear'), 0.0002770689026613197)\n",
      "(('box', 'hill'), 0.0002624863288370397)\n",
      "(('half', 'hour'), 0.0002624863288370397)\n",
      "(('take', 'care'), 0.0002624863288370397)\n",
      "(('young', 'ladies'), 0.0002479037550127598)\n",
      "(('every', 'day'), 0.00023332118118847976)\n",
      "(('last', 'night'), 0.00023332118118847976)\n",
      "(('project', 'gutenberg'), 0.00023332118118847976)\n",
      "(('replied', 'emma'), 0.00023332118118847976)\n",
      "(('well', 'said'), 0.00023332118118847976)\n",
      "(('dear', 'jane'), 0.0002187386073641998)\n",
      "(('let', 'us'), 0.0002187386073641998)\n",
      "(('poor', 'harriet'), 0.0002187386073641998)\n",
      "(('quite', 'well'), 0.0002187386073641998)\n",
      "(('soon', 'afterwards'), 0.0002187386073641998)\n",
      "(('dear', 'sir'), 0.0002041560335399198)\n",
      "(('emma', 'said'), 0.0002041560335399198)\n",
      "(('ever', 'since'), 0.0002041560335399198)\n",
      "(('great', 'pleasure'), 0.0002041560335399198)\n",
      "(('oh', 'woodhouse'), 0.0002041560335399198)\n",
      "(('emma', 'harriet'), 0.0001895734597156398)\n",
      "(('glad', 'see'), 0.0001895734597156398)\n",
      "(('may', 'well'), 0.0001895734597156398)\n"
     ]
    }
   ],
   "source": [
    "# setup to process bigrams\n",
    "from nltk.collocations import *\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "       \n",
    "finder = BigramCollocationFinder.from_words(words)\n",
    "# choose to use both the non-alpha word filter and a stopwords filter\n",
    "#finder.apply_word_filter(alpha_filter)\n",
    "#finder.apply_word_filter(lambda w: w in stopwords)\n",
    "\n",
    "# score by frequency and display the top 50 bigrams\n",
    "scored = finder.score_ngrams(bigram_measures.raw_freq)\n",
    "print ()\n",
    "print (\"Bigrams from file with top 50 frequencies\")\n",
    "for item in scored[:50]:\n",
    "        print (item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-22T01:20:30.115441Z",
     "start_time": "2019-07-22T01:20:30.066556Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trigram_measures' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-104-4602e18a0181>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m# score by frequency and display the top 50 bigrams\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mscored\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfinder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore_ngrams\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrigram_measures\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraw_freq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"Bigrams from file with top 50 frequencies\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'trigram_measures' is not defined"
     ]
    }
   ],
   "source": [
    "# setup to process bigrams\n",
    "from nltk.collocations import *\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "       \n",
    "finder = BigramCollocationFinder.from_words(words)\n",
    "# choose to use both the non-alpha word filter and a stopwords filter\n",
    "#finder.apply_word_filter(alpha_filter)\n",
    "#finder.apply_word_filter(lambda w: w in stopwords)\n",
    "\n",
    "# score by frequency and display the top 50 bigrams\n",
    "scored = finder.score_ngrams(trigram_measures.raw_freq)\n",
    "print ()\n",
    "print (\"Bigrams from file with top 50 frequencies\")\n",
    "for item in scored[:50]:\n",
    "        print (item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caeser Without Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-22T01:08:05.533248Z",
     "start_time": "2019-07-22T01:08:05.352730Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tragedie', 'julius', 'caesar', 'william', 'shakespeare', 'actus', 'primus', 'scoena', 'prima', 'enter', 'flauius', 'murellus', 'certaine', 'commoners', 'ouer', 'stage', 'flauius', 'hence', 'home', 'idle', 'creatures', 'get', 'home', 'holiday', 'know', 'mechanicall', 'ought', 'walke', 'vpon', 'labouring', 'day', 'without', 'signe', 'profession', 'speake', 'trade', 'art', 'thou', 'car', 'sir', 'carpenter', 'mur', 'thy', 'leather', 'apron', 'thy', 'rule', 'dost', 'thou', 'thy', 'best', 'apparrell', 'sir', 'trade', 'cobl', 'truely', 'sir', 'respect', 'fine', 'workman', 'say', 'cobler', 'mur', 'trade', 'art', 'thou', 'answer', 'directly', 'cob', 'trade', 'sir', 'hope', 'may', 'vse', 'safe', 'conscience', 'indeed', 'sir', 'mender', 'bad', 'soules', 'fla', 'trade', 'thou', 'knaue', 'thou', 'naughty', 'knaue', 'trade', 'cobl', 'nay', 'beseech', 'sir', 'yet', 'sir', 'mend', 'mur', 'meanst', 'thou', 'mend']\n"
     ]
    }
   ],
   "source": [
    "#Emma Without Lemmaziation\n",
    "# load data\n",
    "filename = 'shakespeare-caesar.txt'\n",
    "file = open(filename, 'rt')\n",
    "text = file.read()\n",
    "file.close()\n",
    "# split into words\n",
    "from nltk.tokenize import word_tokenize\n",
    "tokens = word_tokenize(text)\n",
    "# convert to lower case\n",
    "tokens = [w.lower() for w in tokens]\n",
    "# remove punctuation from each word\n",
    "import string\n",
    "table = str.maketrans('', '', string.punctuation)\n",
    "stripped = [w.translate(table) for w in tokens]\n",
    "# remove remaining tokens that are not alphabetic\n",
    "words = [word for word in stripped if word.isalpha()]\n",
    "# filter out stop words\n",
    "nltkstopwords = nltk.corpus.stopwords.words('english')\n",
    "morestopwords = ['_','could','would','might','must','need','sha','wo','y',\"'s\",\"'d\",\"'ll\",\"'t\",\"'m\",\"'re\",\"'ve\", \"n't\",\"mrs\",\"mr\",\"miss\"]\n",
    "stopwords = nltkstopwords + morestopwords\n",
    "words2 = [w for w in words if not w in stopwords]\n",
    "print(words2[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-22T01:08:23.033163Z",
     "start_time": "2019-07-22T01:08:22.985293Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Bigrams from file with top 50 frequencies\n",
      "(('let', 'vs'), 0.0014766958929395477)\n",
      "(('wee', 'l'), 0.0013844023996308261)\n",
      "(('mark', 'antony'), 0.0011998154130133825)\n",
      "(('marke', 'antony'), 0.0011075219197046607)\n",
      "(('lord', 'bru'), 0.0010152284263959391)\n",
      "(('thou', 'art'), 0.0010152284263959391)\n",
      "(('art', 'thou'), 0.0008306414397784956)\n",
      "(('brutus', 'cassius'), 0.0008306414397784956)\n",
      "(('caesar', 'caes'), 0.0008306414397784956)\n",
      "(('caesar', 'shall'), 0.0008306414397784956)\n",
      "(('enter', 'brutus'), 0.0008306414397784956)\n",
      "(('noble', 'brutus'), 0.0008306414397784956)\n",
      "(('thou', 'hast'), 0.0008306414397784956)\n",
      "(('good', 'morrow'), 0.0007383479464697738)\n",
      "(('good', 'night'), 0.0007383479464697738)\n",
      "(('haue', 'done'), 0.0007383479464697738)\n",
      "(('antony', 'ant'), 0.0006460544531610522)\n",
      "(('bru', 'good'), 0.0006460544531610522)\n",
      "(('enter', 'lucius'), 0.0006460544531610522)\n",
      "(('shall', 'finde'), 0.0006460544531610522)\n",
      "(('cassi', 'brutus'), 0.0005537609598523304)\n",
      "(('come', 'downe'), 0.0005537609598523304)\n",
      "(('euery', 'man'), 0.0005537609598523304)\n",
      "(('haue', 'seene'), 0.0005537609598523304)\n",
      "(('ides', 'march'), 0.0005537609598523304)\n",
      "(('shall', 'haue'), 0.0005537609598523304)\n",
      "(('thee', 'thou'), 0.0005537609598523304)\n",
      "(('antony', 'shall'), 0.00046146746654360867)\n",
      "(('bru', 'cassius'), 0.00046146746654360867)\n",
      "(('brutus', 'haue'), 0.00046146746654360867)\n",
      "(('caesar', 'haue'), 0.00046146746654360867)\n",
      "(('caesar', 'thou'), 0.00046146746654360867)\n",
      "(('caius', 'cassius'), 0.00046146746654360867)\n",
      "(('caius', 'ligarius'), 0.00046146746654360867)\n",
      "(('decius', 'brutus'), 0.00046146746654360867)\n",
      "(('didst', 'thou'), 0.00046146746654360867)\n",
      "(('dost', 'thou'), 0.00046146746654360867)\n",
      "(('enter', 'antony'), 0.00046146746654360867)\n",
      "(('exeunt', 'enter'), 0.00046146746654360867)\n",
      "(('fell', 'downe'), 0.00046146746654360867)\n",
      "(('great', 'caesar'), 0.00046146746654360867)\n",
      "(('haue', 'beene'), 0.00046146746654360867)\n",
      "(('haue', 'heard'), 0.00046146746654360867)\n",
      "(('honourable', 'men'), 0.00046146746654360867)\n",
      "(('luc', 'lord'), 0.00046146746654360867)\n",
      "(('luc', 'sir'), 0.00046146746654360867)\n",
      "(('messala', 'messa'), 0.00046146746654360867)\n",
      "(('metellus', 'cymber'), 0.00046146746654360867)\n",
      "(('mine', 'owne'), 0.00046146746654360867)\n",
      "(('say', 'brutus'), 0.00046146746654360867)\n"
     ]
    }
   ],
   "source": [
    "# setup to process bigrams\n",
    "from nltk.collocations import *\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "       \n",
    "finder = BigramCollocationFinder.from_words(words2)\n",
    "# choose to use both the non-alpha word filter and a stopwords filter\n",
    "#finder.apply_word_filter(alpha_filter)\n",
    "#finder.apply_word_filter(lambda w: w in stopwords)\n",
    "\n",
    "# score by frequency and display the top 50 bigrams\n",
    "scored = finder.score_ngrams(bigram_measures.raw_freq)\n",
    "print ()\n",
    "print (\"Bigrams from file with top 50 frequencies\")\n",
    "for item in scored[:50]:\n",
    "        print (item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
